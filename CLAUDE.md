# Claude Code Development Guide for TaskPilotAI

**The Evolution of Todo: From Console App to Full-Stack Web Application**

This guide explains how to use Claude Code to implement features across all phases following spec-driven development methodology.

## Phase Selection

- **Phase 1**: In-Memory Python Console App → See branch `phase-1`
- **Phase 2**: Full-Stack Web Application → Current branch `phase-2`
- **Phase 3+**: AI Chatbot, Kubernetes, Cloud Deployment → Future branches

---

## Overview

TaskPilotAI Phase 1 is built using:

- **Spec-Driven Development**: Every feature starts with a detailed specification
- **Test-First (TDD)**: Tests are written BEFORE implementation
- **In-Memory Storage**: No database or persistence (Phase 1 only)
- **Claude Code**: AI-powered code generation based on specs
- **Spec-Kit Plus**: Specification management and organization

---

## Key Principles

### 1. Specs Are the Source of Truth

Before any code is written:

1. Read the feature specification in `/specs/features/`
2. Understand all acceptance criteria
3. Review the data model in `/specs/data-models.md`
4. Clarify any ambiguities before proceeding

**Example**: To implement Add Task, read `/specs/features/01-add-task.md` completely.

### 2. Cannot Write Code Manually

This is non-negotiable. Your job is to:

1. **Refine the specification** until it's clear and unambiguous
2. **Ask Claude Code** to implement based on the spec
3. **Iterate on the spec** if Claude Code's output doesn't match requirements

You do NOT write code directly. Claude Code generates it based on your refined spec.

### 3. Test-First (Red-Green-Refactor)

The TDD workflow:

1. **Red Phase**: Write failing tests based on spec
2. **Green Phase**: Ask Claude Code to implement code that passes tests
3. **Refactor Phase**: Clean up code while maintaining passing tests

### 4. Quality Gates Are Non-Negotiable

All features must pass:

- ✅ Specification exists and is detailed
- ✅ All tests pass (`pytest -v`)
- ✅ Code coverage ≥95% (`pytest --cov=src`)
- ✅ No type errors (`mypy src/`)
- ✅ PEP 8 compliant (`flake8 src/`)
- ✅ README and documentation updated
- ✅ No hardcoded secrets or credentials
- ✅ Runs without warnings

---

## Project Structure for Claude Code

```
TaskPilotAI/
├── .specify/memory/constitution.md     # PROJECT RULES (read first!)
├── README.md                           # Project overview
├── specs/                              # SPECIFICATIONS (reference before coding)
│   ├── overview.md                     # Project scope & structure
│   ├── data-models.md                  # Task entity definition
│   └── features/
│       ├── 01-add-task.md             # Feature 1 spec
│       ├── 02-delete-task.md          # Feature 2 spec
│       ├── 03-update-task.md          # Feature 3 spec
│       ├── 04-view-tasks.md           # Feature 4 spec
│       └── 05-mark-complete.md        # Feature 5 spec
├── src/                                # SOURCE CODE (generated by Claude Code)
│   ├── __init__.py
│   ├── main.py                         # CLI entry point
│   ├── models.py                       # Task data model
│   ├── storage.py                      # In-memory storage
│   └── commands.py                     # Command handlers
├── tests/                              # TESTS (write first!)
│   ├── conftest.py                     # Pytest fixtures
│   ├── test_add_task.py                # Feature 1 tests
│   ├── test_delete_task.py             # Feature 2 tests
│   └── ...
└── pyproject.toml                      # Project config
```

**Key Files to Read FIRST**:
1. `.specify/memory/constitution.md` – PROJECT RULES
2. `README.md` – Project overview
3. `/specs/overview.md` – Scope and structure
4. `/specs/data-models.md` – Data model
5. `/specs/features/XX-feature-name.md` – Feature spec

---

## Workflow for Implementing Each Feature

### Step 1: Read the Specification

```
Read: /specs/features/01-add-task.md completely

Understand:
- User stories and why they matter
- All acceptance criteria (Given-When-Then)
- Edge cases to handle
- Input/output contracts
- Data model requirements
- Error handling requirements
```

### Step 2: Understand the Data Model

```
Read: /specs/data-models.md

Understand:
- Task entity structure
- Field types and constraints
- Storage structure (in-memory Python list)
- ID auto-increment behavior
- Timestamp format (ISO 8601)
```

### Step 3: Write Test Cases (RED Phase)

Ask Claude Code:

```
"Based on /specs/features/01-add-task.md, write comprehensive test cases for add_task in tests/test_add_task.py.

Include:
- Test for creating task with title only
- Test for creating task with title and description
- Test for auto-incremented IDs
- Test for validation errors (empty title, too long)
- Test for ISO 8601 timestamps
- Test for success message with ID

Run pytest -v to verify tests fail (RED phase)"
```

### Step 4: Implement Features (GREEN Phase)

Ask Claude Code:

```
"Implement add_task function in src/storage.py to make failing tests pass.

Requirements from /specs/features/01-add-task.md:
- Accept title (required, 1-200 chars) and description (optional, max 1000)
- Auto-increment task ID
- Store task in memory with ISO 8601 timestamps
- Return created task

Follow Python best practices:
- Type hints for all function parameters and returns
- Docstring explaining what function does
- Input validation with clear error messages

Run pytest -v to verify all tests pass (GREEN phase)"
```

### Step 5: Quality Verification

Ask Claude Code:

```
"Run all quality checks for the add_task feature:

pytest -v --cov=src
mypy src/
flake8 src/ tests/

If any checks fail, fix them. All gates must pass."
```

### Step 6: Document

Update README.md and docstrings if needed. Create PHR (Prompt History Record) for the feature.

---

## How to Reference Specifications

When asking Claude Code to implement features:

### Reference Format
```
Based on /specs/features/XX-feature-name.md:
- What to build (from User Stories section)
- How to build it (from Requirements section)
- Success criteria (from Success Criteria section)
```

### Example
```
"Implement delete_task based on /specs/features/02-delete-task.md:

From User Story 1:
- Remove task by ID
- Return success message
- Maintain ID sequence (don't reuse IDs)

From Functional Requirements:
- Accept --id argument (required, positive integer)
- Validate task exists
- Remove from in-memory storage
- Return error if task not found

Run pytest tests/test_delete_task.py to verify"
```

---

## Module Structure for Claude Code

### src/main.py (CLI Entry Point)

Handles:
- Command-line argument parsing
- Route commands to appropriate handlers
- Output formatting and error handling
- Exit codes (0 for success, 1 for user error, 2 for system error)

**Responsibilities**:
- Parse `python main.py <command> [args]`
- Call appropriate function from `src/commands.py`
- Format and print output
- Handle exceptions gracefully

### src/models.py (Task Data Model)

Defines:
- Task entity structure (dataclass or dict)
- Field types and constraints
- Validation methods
- Type hints

**Key Type**: `Task = Dict[str, Any]` with fields: id, title, description, completed, created_at, updated_at

### src/storage.py (In-Memory Storage)

Manages:
- Module-level `tasks: List[Dict] = []`
- Module-level `next_id: int = 1`
- Functions for all CRUD operations
- ID auto-increment logic

**Functions**:
```python
def add_task(title: str, description: str = "") -> Dict
def delete_task(task_id: int) -> bool
def update_task(task_id: int, title: str = None, description: str = None) -> Dict
def get_task(task_id: int) -> Dict
def list_tasks(status: str = "all") -> List[Dict]
def mark_complete(task_id: int) -> Dict
```

### src/commands.py (Command Handlers)

Implements:
- Argument parsing and validation
- Calls to storage functions
- Error handling and messages
- Output formatting

**Functions**:
- `handle_add(title, description)`
- `handle_delete(task_id)`
- `handle_update(task_id, title, description)`
- `handle_list(status, json_output)`
- `handle_complete(task_id)`

### tests/ (Test Files)

Structure:
```
tests/
├── conftest.py                  # Pytest fixtures
├── test_add_task.py             # Tests for add feature
├── test_delete_task.py          # Tests for delete feature
├── test_update_task.py          # Tests for update feature
├── test_view_tasks.py           # Tests for view feature
└── test_mark_complete.py        # Tests for complete feature
```

**Fixtures** (in conftest.py):
- `empty_storage()` – Reset storage to empty list
- `sample_tasks()` – Create test tasks
- `task_storage()` – Reset storage for each test

---

## Code Quality Requirements

### Type Hints (100% Required)

```python
# GOOD
def add_task(title: str, description: str = "") -> Dict[str, Any]:
    """Create a new task with given title and optional description."""
    pass

# BAD (missing types)
def add_task(title, description=""):
    pass
```

### Docstrings (All Public Functions)

```python
def add_task(title: str, description: str = "") -> Dict[str, Any]:
    """
    Create a new task with the given title and optional description.

    Args:
        title: Task title (required, 1-200 characters)
        description: Optional task description (max 1000 characters)

    Returns:
        Created task object with auto-assigned ID

    Raises:
        ValueError: If title is empty or exceeds 200 characters
        ValueError: If description exceeds 1000 characters
    """
    pass
```

### Error Messages (Consistent Format)

```python
# Always use "Error: " prefix
raise ValueError("Error: Title required (1-200 characters)")

# User-friendly messages
raise ValueError("Error: Task ID 5 not found")

# NOT: "Invalid input" or "Something went wrong"
```

### PEP 8 Compliance

- Max line length: 100 characters
- 4 spaces per indentation (not tabs)
- Class names: PascalCase
- Function names: snake_case
- Constants: UPPER_SNAKE_CASE

---

## Testing Strategy

### Test File Structure

```python
# tests/test_add_task.py
import pytest
from src.storage import add_task
from tests.conftest import empty_storage

class TestAddTask:
    """Tests for add_task feature."""

    def test_add_task_with_title_only(self, empty_storage):
        """User Story 1: Create task with title only."""
        result = add_task("Buy groceries")
        assert result["id"] == 1
        assert result["title"] == "Buy groceries"
        assert result["description"] == ""
        assert result["completed"] is False

    def test_add_task_with_title_and_description(self, empty_storage):
        """User Story 2: Create task with title and description."""
        result = add_task("Buy groceries", "Milk, eggs, bread")
        assert result["id"] == 1
        assert result["title"] == "Buy groceries"
        assert result["description"] == "Milk, eggs, bread"

    def test_auto_increment_ids(self, empty_storage):
        """Test that IDs auto-increment correctly."""
        task1 = add_task("Task 1")
        task2 = add_task("Task 2")
        task3 = add_task("Task 3")
        assert task1["id"] == 1
        assert task2["id"] == 2
        assert task3["id"] == 3

    def test_validation_empty_title(self):
        """Edge case: Empty title should raise error."""
        with pytest.raises(ValueError, match="Title required"):
            add_task("")

    def test_validation_title_too_long(self):
        """Edge case: Title exceeding 200 chars should raise error."""
        long_title = "x" * 201
        with pytest.raises(ValueError, match="max 200 characters"):
            add_task(long_title)
```

### Test Fixtures (conftest.py)

```python
import pytest
from src.storage import tasks, next_id

@pytest.fixture
def empty_storage():
    """Clear storage before each test."""
    tasks.clear()
    next_id = 1
    yield
    tasks.clear()
    next_id = 1

@pytest.fixture
def sample_tasks(empty_storage):
    """Create sample tasks for testing."""
    task1 = add_task("Task 1", "Description 1")
    task2 = add_task("Task 2", "Description 2")
    mark_complete(task2["id"])  # Mark second task complete
    return [task1, task2]
```

---

## Running Quality Checks

### Before Submitting Each Feature

```bash
# 1. Run all tests with coverage
pytest -v --cov=src --cov-report=term-missing

# 2. Type checking
mypy src/

# 3. Style checking
flake8 src/ tests/

# 4. Summary
echo "All quality gates passed!" if all checks pass
```

---

## Conversation with Claude Code

### Good Prompts

```
"Based on /specs/features/01-add-task.md, implement the add_task function
in src/storage.py. Include:
- Input validation (title 1-200 chars, description max 1000)
- Auto-increment ID starting from 1
- ISO 8601 timestamps
- Type hints and docstring
- Proper error messages

Run: pytest tests/test_add_task.py -v"
```

### Bad Prompts

```
"Write code to add tasks" (too vague, no spec reference)
"Make it faster" (vague, no spec requirement)
"Use this pattern I found" (overcomplicate, not from spec)
```

---

## Iterating on Implementation

If Claude Code's output doesn't pass tests:

1. **Analyze the test failure**:
   ```
   What does the test expect?
   What does the code actually do?
   What's the difference?
   ```

2. **Refine the spec** if unclear:
   ```
   "The spec says X, but test expects Y. Let me clarify..."
   ```

3. **Ask Claude Code to fix** with specific feedback:
   ```
   "The add_task function isn't auto-incrementing IDs correctly.
   The test expects ID 1 for first task and ID 2 for second task.
   Currently your code is [specific issue].

   Fix by: [specific solution]"
   ```

---

## Common Gotchas & How to Avoid Them

### 1. ID Reuse After Delete

❌ **Wrong**: Delete task 2, add new task → new task gets ID 2
✅ **Correct**: Delete task 2, add new task → new task gets ID 4 (maintains sequence)

**Spec Reference**: `/specs/features/02-delete-task.md` - "maintain ID sequence"

### 2. Updating Timestamps

❌ **Wrong**: created_at changes when task updated
✅ **Correct**: Only updated_at changes; created_at frozen

**Spec Reference**: `/specs/data-models.md` - "created_at never modified after creation"

### 3. Input Validation

❌ **Wrong**: Accept title longer than 200 chars
✅ **Correct**: Reject with error "Title max 200 characters"

**Spec Reference**: `/specs/data-models.md` - "1-200 characters"

### 4. Error Messages

❌ **Wrong**: `Exception: error`
✅ **Correct**: `ValueError("Error: Task ID 5 not found")`

**Spec Reference**: `.specify/memory/constitution.md` - Error Handling Standards

---

## Phase 1 Feature Implementation Order

1. **Feature 1: Add Task** (/specs/features/01-add-task.md)
   - Foundation: creates tasks, auto-increments IDs
   - Dependencies: none

2. **Feature 2: Delete Task** (/specs/features/02-delete-task.md)
   - Requires: working add_task for testing
   - Key: maintain ID sequence

3. **Feature 3: Update Task** (/specs/features/03-update-task.md)
   - Requires: add_task working
   - Key: preserve created_at, update updated_at

4. **Feature 4: View Task List** (/specs/features/04-view-tasks.md)
   - Requires: add_task, delete_task working
   - Key: table formatting, JSON output

5. **Feature 5: Mark Complete** (/specs/features/05-mark-complete.md)
   - Requires: add_task working
   - Key: toggle status, update timestamp

---

## Getting Help

When stuck:

1. **Re-read the spec**: `/specs/features/XX-feature-name.md`
2. **Check test failures**: `pytest -v` shows what's expected
3. **Reference data model**: `/specs/data-models.md` for structure
4. **Check constitution**: `.specify/memory/constitution.md` for rules

---

## Success Checklist

For each feature:

- [ ] Specification read and understood
- [ ] Tests written first (RED phase)
- [ ] Tests fail initially
- [ ] Code implemented (GREEN phase)
- [ ] All tests pass
- [ ] Coverage ≥95%
- [ ] mypy passes
- [ ] flake8 passes
- [ ] README updated
- [ ] Feature documented in CLAUDE.md

---

## Final Submission

Before submitting Phase 1:

- [ ] All 5 features implemented
- [ ] 100% test pass rate
- [ ] Coverage ≥95%
- [ ] mypy: 0 errors
- [ ] flake8: 0 errors
- [ ] README complete
- [ ] CLAUDE.md complete
- [ ] Working console app demo
- [ ] Git commits with meaningful messages
- [ ] No hardcoded secrets

---

---

## Phase 2: Full-Stack Web Application (Current)

### Phase 2 Overview

Transform Phase 1 console app into a multi-user web application with:
- **Frontend**: Next.js with Better Auth
- **Backend**: FastAPI with JWT
- **Database**: Neon PostgreSQL
- **Points**: 150 | **Due**: December 14, 2025

### Phase 2 Project Structure

```
TaskPilotAI/
├── specs/                   # Spec-Kit Plus specifications
│   ├── overview.md
│   ├── features/
│   │   ├── task-crud.md
│   │   └── authentication.md
│   ├── api/
│   │   └── rest-endpoints.md
│   └── database/
│       └── schema.md
│
├── frontend/                # Next.js 16+ application
│   ├── CLAUDE.md           # Frontend guidelines
│   ├── app/                # App Router
│   │   ├── page.tsx
│   │   ├── dashboard/
│   │   └── layout.tsx
│   ├── components/
│   │   ├── TaskForm.tsx
│   │   └── TaskList.tsx
│   ├── lib/
│   │   ├── auth-client.ts  # Better Auth integration
│   │   └── api.ts          # API client
│   └── package.json
│
├── backend/                 # FastAPI application
│   ├── CLAUDE.md           # Backend guidelines
│   ├── main.py             # FastAPI entry point
│   ├── models.py           # SQLModel: User, Task
│   ├── db.py               # Database connection
│   ├── routes/
│   │   ├── auth.py         # Auth endpoints (Better Auth)
│   │   └── tasks.py        # Task CRUD endpoints
│   ├── requirements.txt
│   └── .env
│
├── .spec-kit/
│   └── config.yaml         # Spec-Kit configuration
│
├── CLAUDE.md               # This file
└── README.md
```

### Phase 2 Key Specifications

**Read these in order:**

1. `@specs/overview.md` – Architecture overview
2. `@specs/database/schema.md` – Database design
3. `@specs/api/rest-endpoints.md` – API contracts
4. `@specs/features/authentication.md` – Auth flow
5. `@specs/features/task-crud.md` – Task operations

### Phase 2 Workflow

#### Step 1: Backend Implementation

```
1. Create SQLModel models (User, Task)
2. Create database connection (db.py)
3. Implement JWT authentication
4. Implement Task CRUD endpoints
5. Write and pass tests
6. Verify all quality gates
```

#### Step 2: Frontend Implementation

```
1. Setup Better Auth SDK
2. Create API client with token handling
3. Build UI components (login, dashboard, tasks)
4. Integrate with backend
5. Write tests
6. Deploy to Vercel
```

#### Step 3: Integration Testing

```
1. User signup flow
2. User login flow
3. Create task via API
4. List user's tasks
5. Update/delete tasks
6. User isolation (can't see other users' tasks)
```

### Phase 2 Quality Gates

All of these must pass:

- [ ] Backend database schema created
- [ ] All 6 REST API endpoints implemented
- [ ] JWT authentication middleware working
- [ ] Frontend and backend integrated
- [ ] Better Auth signup/signin working
- [ ] Task isolation per user enforced
- [ ] Comprehensive tests (unit + integration)
- [ ] Frontend deployed to Vercel
- [ ] Backend running and accessible
- [ ] mypy: 0 errors
- [ ] flake8: 0 errors
- [ ] All tests passing with ≥95% coverage

### Phase 2 Key Differences from Phase 1

| Aspect | Phase 1 | Phase 2 |
|--------|---------|---------|
| **Storage** | In-memory Python list | PostgreSQL database |
| **Persistence** | Lost on app restart | Persistent |
| **Users** | Single user | Multi-user with auth |
| **UI** | CLI arguments | Web UI (React) |
| **API** | None | REST API with JWT |
| **Deployment** | Local CLI | Vercel + Backend server |
| **Database** | None | Neon PostgreSQL |
| **Auth** | None | Better Auth + JWT |

### Phase 2 Implementation Commands

**Backend**:
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload --port 8000
```

**Frontend**:
```bash
cd frontend
npm install
npm run dev
# Visit http://localhost:3000
```

**Database**:
```bash
# Create account at neon.tech
# Copy DATABASE_URL to backend/.env
# SQLModel creates tables automatically on startup
```

### Phase 2 Environment Variables

**Backend** (`.env`):
```
DATABASE_URL=postgresql://user:password@host/database
BETTER_AUTH_SECRET=your-secret-key
JWT_SECRET=your-jwt-secret-key
JWT_EXPIRATION_HOURS=168
```

**Frontend** (`.env.local`):
```
NEXT_PUBLIC_API_URL=http://localhost:8000
```

### Phase 2 API Endpoints Summary

| Method | Endpoint | Purpose |
|--------|----------|---------|
| POST | /api/tasks | Create task |
| GET | /api/tasks | List user's tasks |
| GET | /api/tasks/{id} | Get specific task |
| PUT | /api/tasks/{id} | Update task |
| DELETE | /api/tasks/{id} | Delete task |
| PATCH | /api/tasks/{id}/complete | Toggle completion |

All endpoints require: `Authorization: Bearer <jwt_token>`

### Phase 2 Common Gotchas

1. **User Isolation**: Always filter by `user_id` from JWT token
2. **Token Expiration**: Handle 401 errors gracefully
3. **CORS**: Configure allowed origins for frontend
4. **Timestamps**: Always use UTC ISO 8601 format
5. **Response Format**: All responses must follow standard structure
6. **Validation**: Validate at API level, not just database

---

**Last Updated**: 2025-12-07
**Status**: Phase 2 Specifications Created
**Current Phase**: Phase 2 (Full-Stack Web Application)
**Next**: Implement Backend (FastAPI)

## Active Technologies
- Python 3.11+, TypeScript/React 19 (005-chatkit-integration)
- Neon PostgreSQL (existing Phase 2 database) (005-chatkit-integration)

## Recent Changes
- 005-chatkit-integration: Added Python 3.11+, TypeScript/React 19
